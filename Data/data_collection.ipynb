{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.592784Z",
     "start_time": "2025-10-10T18:47:14.589506Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os \n",
    "import time\n",
    "from scipy.stats import zscore"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Collection (only)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.616740Z",
     "start_time": "2025-10-10T18:47:14.614566Z"
    }
   },
   "source": [
    "# define the tickers symbols\n",
    "\n",
    "# equities = ['AAPL', 'MSFT', 'GOOGL', 'SPY'] #list of equities to download apple, microsoft, google and S&P 500 ETF\n",
    "fx_pairs = ['EURUSD=X', 'GBPUSD=X'] #only using Euro/USD and GBP/USD exchange rates\n",
    "\n",
    "tickers = fx_pairs"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.667838Z",
     "start_time": "2025-10-10T18:47:14.631742Z"
    }
   },
   "source": [
    "#downloading the historical data from yahoo finance\n",
    "\n",
    "data = yf.download(tickers, start='2020-10-03', end='2025-10-09', interval='1d', group_by='ticker', auto_adjust=True, threads=True, actions=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.695444Z",
     "start_time": "2025-10-10T18:47:14.687851Z"
    }
   },
   "source": [
    "# to view the pulled data from the api \n",
    "\n",
    "print(data.head())\n",
    "print(data.tail())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker      EURUSD=X                                                 \\\n",
      "Price           Open      High       Low     Close Volume Dividends   \n",
      "Date                                                                  \n",
      "2020-10-05  1.172196  1.179663  1.171930  1.172044      0       0.0   \n",
      "2020-10-06  1.178828  1.180735  1.176734  1.179245      0       0.0   \n",
      "2020-10-07  1.173737  1.178139  1.172541  1.173764      0       0.0   \n",
      "2020-10-08  1.176554  1.178134  1.173530  1.176747      0       0.0   \n",
      "2020-10-09  1.176706  1.182452  1.175876  1.176700      0       0.0   \n",
      "\n",
      "Ticker                   GBPUSD=X                                       \\\n",
      "Price      Stock Splits      Open      High       Low     Close Volume   \n",
      "Date                                                                     \n",
      "2020-10-05          0.0  1.292808  1.298752  1.290156  1.293109      0   \n",
      "2020-10-06          0.0  1.299207  1.300898  1.292340  1.299258      0   \n",
      "2020-10-07          0.0  1.288477  1.292925  1.285033  1.288328      0   \n",
      "2020-10-08          0.0  1.291656  1.296933  1.289657  1.291756      0   \n",
      "2020-10-09          0.0  1.293929  1.301507  1.292674  1.294046      0   \n",
      "\n",
      "Ticker                             \n",
      "Price      Dividends Stock Splits  \n",
      "Date                               \n",
      "2020-10-05       0.0          0.0  \n",
      "2020-10-06       0.0          0.0  \n",
      "2020-10-07       0.0          0.0  \n",
      "2020-10-08       0.0          0.0  \n",
      "2020-10-09       0.0          0.0  \n",
      "Ticker      EURUSD=X                                                 \\\n",
      "Price           Open      High       Low     Close Volume Dividends   \n",
      "Date                                                                  \n",
      "2025-10-02  1.173406  1.175917  1.168566  1.173406      0       0.0   \n",
      "2025-10-03  1.172512  1.175668  1.171619  1.172512      0       0.0   \n",
      "2025-10-06  1.171248  1.173158  1.165406  1.171248      0       0.0   \n",
      "2025-10-07  1.171029  1.171097  1.165515  1.171029      0       0.0   \n",
      "2025-10-08  1.165460  1.165297  1.160362  1.165460      0       0.0   \n",
      "\n",
      "Ticker                   GBPUSD=X                                       \\\n",
      "Price      Stock Splits      Open      High       Low     Close Volume   \n",
      "Date                                                                     \n",
      "2025-10-02          0.0  1.347782  1.350986  1.340195  1.348018      0   \n",
      "2025-10-03          0.0  1.344592  1.348436  1.342985  1.344664      0   \n",
      "2025-10-06          0.0  1.343346  1.348436  1.341796  1.343382      0   \n",
      "2025-10-07          0.0  1.348436  1.348454  1.339441  1.348490      0   \n",
      "2025-10-08          0.0  1.342102  1.343851  1.338509  1.342102      0   \n",
      "\n",
      "Ticker                             \n",
      "Price      Dividends Stock Splits  \n",
      "Date                               \n",
      "2025-10-02       0.0          0.0  \n",
      "2025-10-03       0.0          0.0  \n",
      "2025-10-06       0.0          0.0  \n",
      "2025-10-07       0.0          0.0  \n",
      "2025-10-08       0.0          0.0  \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.717044Z",
     "start_time": "2025-10-10T18:47:14.714698Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "print(os.path.isdir(\"/Users/akilfiros/Desktop/Projects/Side Projects /Quant-Backtesting/Data\"))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.753340Z",
     "start_time": "2025-10-10T18:47:14.737043Z"
    }
   },
   "source": [
    "#saving the data to a csv file\n",
    "\n",
    "data.reset_index(inplace=True) #by doing this reset_index() I am telling pandas to take whatever was previously acting as the index (Date) and move it into a proper column.\n",
    "data.columns = [f\"{col[0]}.{col[1]}\" for col in data.columns]\n",
    "data.to_csv(\"/Users/akilfiros/Desktop/Projects/Side Projects /Quant-Backtesting/Data/market_data_not_cleaned.csv\",index=False,mode='w')\n",
    "\n",
    "#what happens with index=False and mode='w' included in the above line is that now when you're running the notebook from the start again the pulled data from yfinance which is being saved into the csv file will not create a new csv file with the same name instead it rewrite the data in the csv file and only the data will be updated.\n",
    "#the index=False makes sure that pandas does not create an extra unnamed 0 column when writing the CSV\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Cleaning (only)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.775105Z",
     "start_time": "2025-10-10T18:47:14.766932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loading the csv file created and then reading into it\n",
    "\n",
    "df = pd.read_csv('/Users/akilfiros/Desktop/Projects/Side Projects /Quant-Backtesting/Data/market_data_not_cleaned.csv', header= 1)\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2020-10-05  1.172195553779602  1.1796625852584839  1.1719304323196411  \\\n",
      "0  2020-10-06           1.178828            1.180735            1.176734   \n",
      "1  2020-10-07           1.173737            1.178139            1.172541   \n",
      "2  2020-10-08           1.176554            1.178134            1.173530   \n",
      "3  2020-10-09           1.176706            1.182452            1.175876   \n",
      "4  2020-10-12           1.181335            1.182732            1.178800   \n",
      "\n",
      "   1.1720443964004517  0  0.0  0.0.1  1.292808175086975  1.298751950263977  \\\n",
      "0            1.179245  0  0.0    0.0           1.299207           1.300898   \n",
      "1            1.173764  0  0.0    0.0           1.288477           1.292925   \n",
      "2            1.176747  0  0.0    0.0           1.291656           1.296933   \n",
      "3            1.176700  0  0.0    0.0           1.293929           1.301507   \n",
      "4            1.181684  0  0.0    0.0           1.302932           1.307976   \n",
      "\n",
      "   1.2901561260223389  1.2931090593338013  0.1  0.0.2  0.0.3  \n",
      "0            1.292340            1.299258    0    0.0    0.0  \n",
      "1            1.285033            1.288328    0    0.0    0.0  \n",
      "2            1.289657            1.291756    0    0.0    0.0  \n",
      "3            1.292674            1.294046    0    0.0    0.0  \n",
      "4            1.300559            1.303169    0    0.0    0.0  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.798689Z",
     "start_time": "2025-10-10T18:47:14.794394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#handling/checking missing values\n",
    "missing_values = df.isnull().sum().sum()\n",
    "print(\"Missing values:\",missing_values)\n",
    "\n",
    "if missing_values >0:\n",
    "    df_cleaned = df.dropna()\n",
    "    print(f\"Number of missing values: {missing_values}\")\n",
    "else :\n",
    "    print(\"No missing values\")\n",
    "\n",
    "print(df.isnull().sum()) #verify that there is no missing values\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0\n",
      "No missing values\n",
      "2020-10-05            0\n",
      "1.172195553779602     0\n",
      "1.1796625852584839    0\n",
      "1.1719304323196411    0\n",
      "1.1720443964004517    0\n",
      "0                     0\n",
      "0.0                   0\n",
      "0.0.1                 0\n",
      "1.292808175086975     0\n",
      "1.298751950263977     0\n",
      "1.2901561260223389    0\n",
      "1.2931090593338013    0\n",
      "0.1                   0\n",
      "0.0.2                 0\n",
      "0.0.3                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.819899Z",
     "start_time": "2025-10-10T18:47:14.814386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#identify duplicates\n",
    "duplicate_count = df.duplicated().sum() #this returns the number of duplicated rows\n",
    "print(\"Duplicated rows: \", duplicate_count)\n",
    "# if duplicated rows is 0 then that means there is no duplicated rows in the data.\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    df=df.drop_duplicates()\n",
    "    print(f\"Dropped {duplicate_count} duplicate rows\")\n",
    "else:\n",
    "    print(\"No duplicate rows found\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows:  0\n",
      "No duplicate rows found\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.843621Z",
     "start_time": "2025-10-10T18:47:14.835173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#handling outliers\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# \"\"\"\n",
    "# What it does: selects the names of columns in df whose dtype is numeric (int/float).\n",
    "#\n",
    "# Return value / type: a pandas.Index of column names (e.g. Index(['A', 'B', 'C'])).\n",
    "#\n",
    "# Why: you only want to compute z-scores on numeric data (prices, volumes, etc.), not on string columns like Ticker or Date.\n",
    "#\n",
    "# Pitfall: if there are no numeric columns this will be empty — later code will fail. Also ensure numeric columns are actually numeric (no stray strings).\n",
    "# \"\"\"\n",
    "\n",
    "z_scores = np.abs(zscore(df[numeric_cols]))\n",
    "# \"\"\"\n",
    "# What it does, step by step:\n",
    "#\n",
    "# 1. df[numeric_cols] selects a DataFrame of numeric columns.\n",
    "#\n",
    "# 2. zscore(...) (from scipy.stats) computes the z-score for each value = (x - mean_column) / std_column. By default axis=0 so the mean/std are computed per column.\n",
    "#\n",
    "# 3. np.abs(...) takes the absolute value of the z-scores so that very negative and very positive deviations are treated the same.\n",
    "#\n",
    "# Return value / type: z_scores is a NumPy array (not a DataFrame) with the same shape as df[numeric_cols] — i.e. (n_rows, n_numeric_cols).\n",
    "#\n",
    "# Notes:\n",
    "#\n",
    "# - zscore uses ddof=0 by default (population std). That affects the scale slightly vs pandas.std() default (which uses ddof=1).\n",
    "#\n",
    "# - If a column has zero variance (std = 0), zscore will produce NaN or inf. You should handle constant columns first.\n",
    "# \"\"\"\n",
    "\n",
    "#defining the threshold\n",
    "threshold = 3\n",
    "# \"\"\"\n",
    "# sets the cutoff for flagging outliers. A z-score > 3 (or < -3) is a common “3-sigma” rule of thumb, meaning: values with absolute z-score above 3 are considered extreme relative to that column’s distribution.\n",
    "# \"\"\"\n",
    "\n",
    "outlier_mask = (z_scores > threshold)\n",
    "# \"\"\"\n",
    "# What it does: creates a boolean array with the same shape as z_scores, where True indicates that the absolute z-score for that cell exceeds the threshold.\n",
    "#\n",
    "# Type / shape: NumPy boolean array (n_rows, n_numeric_cols).\n",
    "#\n",
    "# Example :\n",
    "# [[False, False, True],\n",
    "#  [False, False, False],\n",
    "#  [True, False, False]]\n",
    "#  means row 0, col 2 is an outlier; row 2, col 0 is an outlier.\n",
    "# \"\"\"\n",
    "\n",
    "outlier_rows = np.where(outlier_mask)[0]\n",
    "# \"\"\"\n",
    "# What np.where(outlier_mask) returns: a tuple (rows, cols) of 1-D arrays of indices where outlier_mask is True.\n",
    "#\n",
    "# Taking [0]: selects the row indices of every flagged cell. So outlier_rows becomes a 1-D NumPy array containing row indices — one entry for each flagged cell. If the same row had outliers in multiple columns it will appear multiple times.\n",
    "#\n",
    "# Example: if outlier_mask has True at (0,2) and (2,0) then np.where(outlier_mask) is (array([0,2]), array([2,0])) and [0] gives array([0,2]).\n",
    "# \"\"\"\n",
    "\n",
    "print(f\"Potential outlier rows: {len(np.unique(outlier_rows))}\")\n",
    "# \"\"\"\n",
    "# prints the number of unique rows that contain at least one outlier, by taking np.unique of outlier_rows (so duplicates are removed), then taking the length.\n",
    "#\n",
    "# Why np.unique used here: because outlier_rows can list the same row multiple times (once per column that flagged it).\n",
    "# \"\"\"\n",
    "\n",
    "if len(outlier_rows) > 0:\n",
    "    df = df[(z_scores < threshold).all(axis=1)]\n",
    "    print(f\"Removed rows with extreme outliers\")\n",
    "else :\n",
    "    print(\"No extreme outlier found\")\n",
    "# \"\"\"\n",
    "# Filtering df: df[(z_scores < threshold).all(axis=1)] :\n",
    "# (z_scores < threshold) is a boolean array where True = cell z-score < threshold.\n",
    "#\n",
    "# .all(axis=1) reduces per-row and returns a 1-D boolean array of length n_rows where True means every numeric column in that row has |z| < threshold. In other words: keep rows where no numeric column exceeds threshold.\n",
    "#\n",
    "# df[...] uses that boolean vector to filter rows in the DataFrame; rows with any flagged outlier are dropped.\n",
    "#\n",
    "# Effect: you remove any row that had at least one numeric column with |z| ≥ threshold.\n",
    "# \"\"\"\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential outlier rows: 4\n",
      "Removed rows with extreme outliers\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.870434Z",
     "start_time": "2025-10-10T18:47:14.864341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Reset index and ensure numeric types\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "print(\"\\n Data cleaning complete!\")\n",
    "print(df.info())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data cleaning complete!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   2020-10-05          0 non-null      object \n",
      " 1   1.172195553779602   0 non-null      float64\n",
      " 2   1.1796625852584839  0 non-null      float64\n",
      " 3   1.1719304323196411  0 non-null      float64\n",
      " 4   1.1720443964004517  0 non-null      float64\n",
      " 5   0                   0 non-null      float64\n",
      " 6   0.0                 0 non-null      float64\n",
      " 7   0.0.1               0 non-null      float64\n",
      " 8   1.292808175086975   0 non-null      float64\n",
      " 9   1.298751950263977   0 non-null      float64\n",
      " 10  1.2901561260223389  0 non-null      float64\n",
      " 11  1.2931090593338013  0 non-null      float64\n",
      " 12  0.1                 0 non-null      float64\n",
      " 13  0.0.2               0 non-null      float64\n",
      " 14  0.0.3               0 non-null      float64\n",
      "dtypes: float64(14), object(1)\n",
      "memory usage: 132.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:47:14.903965Z",
     "start_time": "2025-10-10T18:47:14.888646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#saving the data to a csv file\n",
    "\n",
    "data.to_csv(\"/Users/akilfiros/Desktop/Projects/Side Projects /Quant-Backtesting/Data/market_data_cleaned.csv\",index=False,mode='w')"
   ],
   "outputs": [],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python_env_3.11.10",
   "language": "python",
   "name": "conda_python_env_3.11.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
